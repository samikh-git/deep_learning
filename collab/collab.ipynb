{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42b654ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our libraries that we need; currently hidden\n",
    "# ! pip install torch pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a86c4317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of our import statements to have them all\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dbfe45",
   "metadata": {},
   "source": [
    "# Creating a Collaborative Filtering Recommendation System Using The Movie Lens Dataset\n",
    "\n",
    "Today we will learn about latent factors, embeddings, collaborative filtering, and recommendation systems using the Movie Lens 100k dataset (find more details at this [link](https://grouplens.org/datasets/movielens/))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533114eb",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8229c18e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "      <td>Heat (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "      <td>Seven (a.k.a. Se7en) (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "      <td>Usual Suspects, The (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp                        title\n",
       "0       1        1     4.0  964982703             Toy Story (1995)\n",
       "1       1        3     4.0  964981247      Grumpier Old Men (1995)\n",
       "2       1        6     4.0  964982224                  Heat (1995)\n",
       "3       1       47     5.0  964983815  Seven (a.k.a. Se7en) (1995)\n",
       "4       1       50     5.0  964982931   Usual Suspects, The (1995)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ratings = pd.read_csv(\"./movie-lens-dataset/ratings.csv\")\n",
    "movies = pd.read_csv(\"./movie-lens-dataset/movies.csv\")\n",
    "\n",
    "collab = pd.merge(ratings, movies, on = \"movieId\").drop(columns = [\"genres\"])\n",
    "collab.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28da7830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>100836.000000</td>\n",
       "      <td>1.008360e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>326.127564</td>\n",
       "      <td>19435.295718</td>\n",
       "      <td>3.501557</td>\n",
       "      <td>1.205946e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>182.618491</td>\n",
       "      <td>35530.987199</td>\n",
       "      <td>1.042529</td>\n",
       "      <td>2.162610e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8.281246e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>177.000000</td>\n",
       "      <td>1199.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.019124e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>2991.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.186087e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>477.000000</td>\n",
       "      <td>8122.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.435994e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>610.000000</td>\n",
       "      <td>193609.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.537799e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              userId        movieId         rating     timestamp\n",
       "count  100836.000000  100836.000000  100836.000000  1.008360e+05\n",
       "mean      326.127564   19435.295718       3.501557  1.205946e+09\n",
       "std       182.618491   35530.987199       1.042529  2.162610e+08\n",
       "min         1.000000       1.000000       0.500000  8.281246e+08\n",
       "25%       177.000000    1199.000000       3.000000  1.019124e+09\n",
       "50%       325.000000    2991.000000       3.500000  1.186087e+09\n",
       "75%       477.000000    8122.000000       4.000000  1.435994e+09\n",
       "max       610.000000  193609.000000       5.000000  1.537799e+09"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e53744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userId\n",
       "1       232\n",
       "2        29\n",
       "3        39\n",
       "4       216\n",
       "5        44\n",
       "       ... \n",
       "606    1115\n",
       "607     187\n",
       "608     831\n",
       "609      37\n",
       "610    1302\n",
       "Length: 610, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_user_ID = collab.groupby(\"userId\").size()\n",
    "grouped_by_user_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c98a22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_by_movie_ID = collab.groupby(\"movieId\").count() \n",
    "len(grouped_by_movie_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eee3549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The distribution of ratings for the movies in our dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "collab['rating'].hist()\n",
    "print(\"The distribution of ratings for the movies in our dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948fb3f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>movieId</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>193565</th>\n",
       "      <th>193567</th>\n",
       "      <th>193571</th>\n",
       "      <th>193573</th>\n",
       "      <th>193579</th>\n",
       "      <th>193581</th>\n",
       "      <th>193583</th>\n",
       "      <th>193585</th>\n",
       "      <th>193587</th>\n",
       "      <th>193609</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606</th>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>610 rows Ã— 9724 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "movieId  1       2       3       4       5       6       7       8       \\\n",
       "userId                                                                    \n",
       "1           4.0     NaN     4.0     NaN     NaN     4.0     NaN     NaN   \n",
       "2           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "3           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "4           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "5           4.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "606         2.5     NaN     NaN     NaN     NaN     NaN     2.5     NaN   \n",
       "607         4.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "608         2.5     2.0     2.0     NaN     NaN     NaN     NaN     NaN   \n",
       "609         3.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "610         5.0     NaN     NaN     NaN     NaN     5.0     NaN     NaN   \n",
       "\n",
       "movieId  9       10      ...  193565  193567  193571  193573  193579  193581  \\\n",
       "userId                   ...                                                   \n",
       "1           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "2           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "3           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "4           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "5           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "...         ...     ...  ...     ...     ...     ...     ...     ...     ...   \n",
       "606         NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "607         NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "608         NaN     4.0  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "609         NaN     4.0  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "610         NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
       "\n",
       "movieId  193583  193585  193587  193609  \n",
       "userId                                   \n",
       "1           NaN     NaN     NaN     NaN  \n",
       "2           NaN     NaN     NaN     NaN  \n",
       "3           NaN     NaN     NaN     NaN  \n",
       "4           NaN     NaN     NaN     NaN  \n",
       "5           NaN     NaN     NaN     NaN  \n",
       "...         ...     ...     ...     ...  \n",
       "606         NaN     NaN     NaN     NaN  \n",
       "607         NaN     NaN     NaN     NaN  \n",
       "608         NaN     NaN     NaN     NaN  \n",
       "609         NaN     NaN     NaN     NaN  \n",
       "610         NaN     NaN     NaN     NaN  \n",
       "\n",
       "[610 rows x 9724 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collab.pivot(index = \"userId\", columns = \"movieId\", values = \"rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b778d49",
   "metadata": {},
   "source": [
    "The above pivot table shows the rating that each user assigned to each movie in the dataset. Although we see a lots of ```NaN```, this is probably a better visualization of what is happening in our data and will provide a better way of thinking about our data when creating our collaborative filter.\n",
    "\n",
    "Our objective is to create a model that will be able to fill in those blanks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650eb639",
   "metadata": {},
   "source": [
    "### Preparing our Data for Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "551f4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch import tensor\n",
    "\n",
    "input_features = [\"userId\", \"movieId\"]\n",
    "target_feature = \"rating\"\n",
    "\n",
    "train_xs_df = collab[input_features]\n",
    "train_y_df = collab[target_feature]\n",
    "\n",
    "train_xs_tns, train_y_tns = tensor(train_xs_df.values, dtype=torch.int32), tensor(train_y_df.values, dtype=torch.float32)\n",
    "\n",
    "train_size = int(len(train_xs_tns) * 0.8)\n",
    "valid_size = len(train_xs_tns) - train_size\n",
    "\n",
    "train = list(zip(train_xs_tns, train_y_tns))\n",
    "train_s, valid_s = random_split(train, [train_size, valid_size])\n",
    "\n",
    "train_dl, valid_dl = DataLoader(train_s, batch_size = 16, shuffle = True), DataLoader(valid_s, batch_size = 16, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9426000b",
   "metadata": {},
   "source": [
    "## Building the Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45edfc39",
   "metadata": {},
   "source": [
    "### Latent Factors\n",
    "\n",
    "One common way to approach this problem is the following. Let's say that we have a user with the following preference vector: \n",
    "\n",
    "$$ \\vec{p} = \\langle 0.2, 0.9, 0.3 \\rangle $$\n",
    "\n",
    "With $p_1$ being his preference for a movie that is action-packed, $p_2$ being his preference for movie that is under 1h30 long, and $p_3$ being his preference for old movies. \n",
    "\n",
    "If we have a movie that has the following criteria for his preferences: \n",
    "\n",
    "$$ \\vec{m} = \\langle 0.3, 0.7, 0.2 \\rangle$$ \n",
    "\n",
    "We can define the user's preference to this movie by taking the dot product of the user's preference vector $\\vec{p}$ and the movie's vector $\\vec{m}$. \n",
    "\n",
    "We can use that dot product as our prediction. Thus: \n",
    "\n",
    "$$ \\text{prediction} = \\vec{p} \\cdot \\vec{m} $$\n",
    "\n",
    "One very important nuance. We do not know these vectors for each user and each movie. However, using stochastic gradient descent, we can start with random weights and find the weights that provide the right prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c60cf7",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "An embedding is a table that allows us to look up the latent factors for each item. These are very important in creating our model as we cannot do the simple lookup as we can do in the pivot table we've built in the previous cells.\n",
    "\n",
    "In the next cell, we are creating a an embedding for 5 items and 2 latent factors per item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8bbeb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0181,  0.0076],\n",
       "        [-0.0004,  0.0099],\n",
       "        [-0.0128,  0.0038],\n",
       "        [ 0.0041,  0.0052],\n",
       "        [ 0.0147,  0.0043]], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_embedding(size):\n",
    "    return nn.Parameter(torch.zeros(*size).normal_(0, 0.01))\n",
    "\n",
    "create_embedding([5,2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed48bc1",
   "metadata": {},
   "source": [
    "### Defining Our Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c72b478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret(prediction):\n",
    "    \"\"\" A function to interpret the predictions of our model -- rounds a float to the closest 0.5\"\"\"\n",
    "    pred = round(prediction, 1)\n",
    "    digit = int(pred)\n",
    "    decimal = round(pred-digit,1)\n",
    "    if decimal > 0.25 and decimal < 0.75: \n",
    "        return digit + 0.5\n",
    "    elif decimal < 0.25: \n",
    "        return float(digit)\n",
    "    else: \n",
    "        return digit + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ce9797",
   "metadata": {},
   "source": [
    "Our ranged sigmoid will help us make sure that our model's outputs are ranged between 0 and 5. This will help improve our model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cec7652",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def ranged_sigmoid(x, lower_bound, upper_bound):\n",
    "  \"\"\"Creates a ranged sigmoid function given a certain lower bound and upper bound\"\"\"\n",
    "  sigmoid_output = F.sigmoid(x)\n",
    "  scaled_output = sigmoid_output  * (upper_bound - lower_bound) + lower_bound\n",
    "  return scaled_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8986b7d5",
   "metadata": {},
   "source": [
    "We have defined a basic Dot Product model (called *Probabilistic Matrix Factorization (PMF)*).\n",
    "\n",
    "We have added bias vectors (a scalar) for each user and mvoie that we can add to our dot product to make sure to account to the fact that certain users will always rate every movie higher/lower consistently and certain movies are rated higher/lower by everyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341b4107",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DotProduct(nn.Module):\n",
    "    def __init__(self, n_users, n_movies,n_factors, y_range = (0, 5.5)):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
    "        self.user_bias = nn.Embedding(n_users, 1)\n",
    "        self.movie_factors = nn.Embedding(n_movies, n_factors)\n",
    "        self.movie_bias = nn.Embedding(n_movies, 1)\n",
    "        self.y_range = (0, 5.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        users = self.user_factors(x[:,0])\n",
    "        movies = self.movie_factors(x[:,1])\n",
    "        product = (users*movies).sum(dim = 1, keepdim = True)\n",
    "        product += self.user_bias(x[:,0]) + self.movie_bias(x[:,1])\n",
    "        return ranged_sigmoid(product, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e19e2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = 611\n",
    "n_movies = 193610\n",
    "n_factors = 5\n",
    "\n",
    "dot_product_model = DotProduct(n_users, n_movies, n_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de354a58",
   "metadata": {},
   "source": [
    "### Training Loop \n",
    "\n",
    "We have our standard training loop using Stochastic Gradient Descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "770d7d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:12: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:12: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:24: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:24: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train Mean Loss: 1.495 | Valid Mean Loss: 1.401 | Valid Mean Epoch Accuracy: 0.2 \n",
      "Epoch #2 | Train Mean Loss: 1.401 | Valid Mean Loss: 1.396 | Valid Mean Epoch Accuracy: 0.2 \n",
      "Epoch #3 | Train Mean Loss: 1.401 | Valid Mean Loss: 1.406 | Valid Mean Epoch Accuracy: 0.202 \n",
      "Epoch #4 | Train Mean Loss: 1.401 | Valid Mean Loss: 1.406 | Valid Mean Epoch Accuracy: 0.201 \n",
      "Epoch #5 | Train Mean Loss: 1.402 | Valid Mean Loss: 1.4 | Valid Mean Epoch Accuracy: 0.199 \n",
      "Epoch #6 | Train Mean Loss: 1.402 | Valid Mean Loss: 1.403 | Valid Mean Epoch Accuracy: 0.201 \n",
      "Epoch #7 | Train Mean Loss: 1.401 | Valid Mean Loss: 1.398 | Valid Mean Epoch Accuracy: 0.199 \n",
      "Epoch #8 | Train Mean Loss: 1.402 | Valid Mean Loss: 1.402 | Valid Mean Epoch Accuracy: 0.201 \n",
      "Epoch #9 | Train Mean Loss: 1.401 | Valid Mean Loss: 1.408 | Valid Mean Epoch Accuracy: 0.201 \n",
      "Epoch #10 | Train Mean Loss: 1.402 | Valid Mean Loss: 1.407 | Valid Mean Epoch Accuracy: 0.201 \n",
      "Training complete :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DotProduct(\n",
       "  (user_factors): Embedding(611, 5)\n",
       "  (user_bias): Embedding(611, 1)\n",
       "  (movie_factors): Embedding(193610, 5)\n",
       "  (movie_bias): Embedding(193610, 1)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "loss_f = F.mse_loss \n",
    "SGD = torch.optim.SGD(params = dot_product_model.parameters(), lr = 0.1, weight_decay = 0.05)\n",
    "\n",
    "def train_model(model, loss_f, optimizer): \n",
    "    def train_epoch():\n",
    "        losses = []\n",
    "        for xb, yb in train_dl:\n",
    "            preds = model.forward(xb)\n",
    "            loss = loss_f(preds, yb)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        return np.mean(np.array(losses))\n",
    "\n",
    "    def validate_epoch():\n",
    "        accuracies = []\n",
    "        losses = []\n",
    "        for xb, yb in valid_dl:\n",
    "            preds = model.forward(xb)\n",
    "            loss = loss_f(preds, yb)\n",
    "            losses.append(loss.item())\n",
    "            batch_accuracy = (preds.detach().apply_(interpret) == yb).float().mean().item()\n",
    "            accuracies.append(batch_accuracy)\n",
    "        return np.mean(np.array(accuracies)), np.mean(np.array(losses))\n",
    "\n",
    "    for ep in range(10):\n",
    "        mean_loss = train_epoch()\n",
    "        valid_accuracy, valid_loss = validate_epoch()\n",
    "        print(f\"Epoch #{ep+1} | Train Mean Loss: {round(mean_loss, 3)} | Valid Mean Loss: {round(valid_loss, 3)} | Valid Mean Epoch Accuracy: {round(valid_accuracy, 3)} \")\n",
    "    \n",
    "    print(\"Training complete :)\")\n",
    "    return model\n",
    "\n",
    "train_model(dot_product_model, loss_f, SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a591e",
   "metadata": {},
   "source": [
    "### Weight Decay\n",
    "\n",
    "Weight decay (also known as *L2 Regularization*) is a regularization technique that allows us to avoid overfitting in our model.\n",
    "\n",
    "The way that we do this is the following. We first set a weight decay factor, which we will call `wd`. We will then multiply our weight decay factor by the sum of the squares of our parameters. Then we will add this product to our loss function. \n",
    "\n",
    "In PyTorch,\n",
    "\n",
    "```python\n",
    "    wd_loss = loss + wd * (parameters**2).sum()\n",
    "```\n",
    "\n",
    "This can be very computationally ineffecient as we need to calculate a sum of our parameters. We can use this analogous method when calculating our gradients.\n",
    "\n",
    "```python\n",
    "    parameters.grad += wd*2*parameters\n",
    "```\n",
    "\n",
    "The intuition behind weight decay is that we are hindering the training process of our model. By increasing the loss that we calculate, we are making our model calculate lower gradients. This will help our model generalize more effectively, thus preventing overfitting.\n",
    "\n",
    "We are able to define weight decay factor in our SGD optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4284c2",
   "metadata": {},
   "source": [
    "### Neural Network Approach\n",
    "\n",
    "Instead of using the dotproduct of our embeddings, we will use the a neural network. As inputs, our network will take a concatination of our embedding vectors \n",
    "\n",
    "**Symbolically**\n",
    "\n",
    "Let $e_1 = \\langle e_{1.1}, e_{1.2}, e_{1.3} \\rangle$ and $e_2 = \\langle e_{2.1}, e_{2.2}, e_{2.3} \\rangle$. Concatinating our two vectors amount to doing the following operation: \n",
    "\n",
    "$$ \\text{cat}(e_1, e_2) = \\langle e_{1.1}, e_{1.2}, e_{1.3}, e_{2.1}, e_{2.2}, e_{2.3} \\rangle $$\n",
    "\n",
    "*Side note*\n",
    "\n",
    "I need to use the math to visualize these torch operations better. The operations are still a little abstract for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b462d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollabNN(nn.Module):\n",
    "    def __init__(self, n_users, n_user_factors, n_movies, n_movie_factors, y_range = (0,5.5), n_act = 100):\n",
    "        super().__init__()\n",
    "        self.user_factors = nn.Embedding(n_users, n_user_factors)\n",
    "        self.movie_factors = nn.Embedding(n_movies, n_movie_factors)\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_user_factors + n_movie_factors, n_act), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_act, 1)\n",
    "        )\n",
    "        self.y_range = y_range\n",
    "\n",
    "    def forward(self, x):\n",
    "        embs = self.user_factors(x[:,0]), self.movie_factors(x[:,1])\n",
    "        pred = self.layers(torch.cat(embs, dim = 1))\n",
    "        return ranged_sigmoid(pred, *self.y_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8338fc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:12: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:12: UserWarning: Using a target size (torch.Size([12])) that is different to the input size (torch.Size([12, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:24: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n",
      "/var/folders/9_/kcr9skyn5bx1npgv347g5ngm0000gn/T/ipykernel_82818/999512264.py:24: UserWarning: Using a target size (torch.Size([8])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = loss_f(preds, yb)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1 | Train Mean Loss: 1.1 | Valid Mean Loss: 1.143 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #2 | Train Mean Loss: 1.098 | Valid Mean Loss: 1.112 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #3 | Train Mean Loss: 1.099 | Valid Mean Loss: 1.098 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #4 | Train Mean Loss: 1.099 | Valid Mean Loss: 1.097 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #5 | Train Mean Loss: 1.098 | Valid Mean Loss: 1.095 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #6 | Train Mean Loss: 1.1 | Valid Mean Loss: 1.1 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #7 | Train Mean Loss: 1.099 | Valid Mean Loss: 1.096 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #8 | Train Mean Loss: 1.099 | Valid Mean Loss: 1.115 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #9 | Train Mean Loss: 1.099 | Valid Mean Loss: 1.108 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Epoch #10 | Train Mean Loss: 1.098 | Valid Mean Loss: 1.101 | Valid Mean Epoch Accuracy: 0.13 \n",
      "Training complete :)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CollabNN(\n",
       "  (user_factors): Embedding(611, 72)\n",
       "  (movie_factors): Embedding(193610, 103)\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=175, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_users = 611\n",
    "n_movies = 193610\n",
    "n_user_factors = 72\n",
    "n_movie_factors = 103\n",
    "\n",
    "nn_model = CollabNN(n_users, n_user_factors, n_movies, n_movie_factors)\n",
    "\n",
    "sgd = torch.optim.SGD(params = nn_model.parameters(), lr = 0.1, weight_decay = 0.05)\n",
    "train_model(nn_model, loss_f, sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b486ba5",
   "metadata": {},
   "source": [
    "When creating our collaborative filtering model, we need to be careful about certain parts. \n",
    "\n",
    "I will cite the Fast AI book: \n",
    "\n",
    "> The biggest challenge with using collaborative filtering models in practice is the *bootstrapping problem*. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What products do you recommend to your very first user? \n",
    "\n",
    ">But even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of *use your common sense*. You could assign new users the mean of all of the embedding vectors of your other users, but this has the problem that that particular combination of latent factors may be not at all common (for instance, the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent *average taste*.\n",
    "\n",
    ">Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them that could help you to understand their tastes. Then you can create a model where the dependent variable is a user's embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup metadata. We will see in the next section how to create these kinds of tabular models. (You may have noticed that when you sign up for services such as Pandora and Netflix, they tend to ask you a few questions about what genres of movie or music you like; this is how they come up with your initial collaborative filtering recommendations.)\n",
    "\n",
    ">One thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don't watch very much else, and spend a lot of time putting their ratings on websites. As a result, anime tends to be heavily overrepresented in a lot of *best ever movies* lists. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all.\n",
    "\n",
    ">Such a problem can change the entire makeup of your user base, and the behavior of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This type of bias has a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorated in such a way that they expressed values at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly and in a way that is hidden until it is too late.\n",
    "\n",
    ">In a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify up front how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It's all about ensuring that there are humans in the loop; that there is careful monitoring, and a gradual and thoughtful rollout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
