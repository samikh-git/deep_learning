{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df375ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U huggingface_hub\n",
    "! python -m pip install --upgrade 'transformers>=4.49.0' datasets timm\n",
    "! pip install -U pandas numpy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109f8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-07T02:00:24.623494Z",
     "start_time": "2025-06-07T02:00:24.526331Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fef913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, sys\n",
    "print(\"Version:\", transformers.__version__)\n",
    "print(\"Location:\", transformers.__file__)\n",
    "print(\"Python executable:\", sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# NLP - RNNs, Transformers, Hugging Face\n",
    "\n",
    "In this notebook, we will be understanding and delving more deeply into NLP. Currently, NLP is the most popular application of deep learning. All Large Language Models (LLMs) currently operate based on the transformer architecture to provide generative capabilities.\n",
    "\n",
    "We will try to make a NLP classification model that can identify Charles Dickens' writings.\n",
    "\n",
    "This notebook is based off of module 4 of Fast AI's on NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951d672d0fa9baf4",
   "metadata": {},
   "source": [
    "### Language Models\n",
    "\n",
    "A language model is a model that is trained to predict the next word in a text based on the previous words. A language model uses something called self-supervised training to achieve this. Without external labels, it can find labels within the text it needs to evaluate. To achieve this, our language model needs to develop a certain understanding of the language. This means that for our applications, our language model needs to understand the English language, the French language, the German language, etc.\n",
    "\n",
    "An example of an application development process is the IMDb review classifier. We will use a language model that was trained on Wikipedia data. Unfortunately, this model might not be entirely suitable for IMDb review English. Wikipedia articles are usually written in a different style and format from an IMDb review. In order to get accurate classifications, we ought to fine-tune our model on IMDb English. From that fine-tuned model, we can then work on developing a classification model for IMDb movie reviews that will be very accurate.\n",
    "\n",
    "The preceding process is called the Universal Language Model Fine-Tuning Process (ULMFit).\n",
    "\n",
    "#### Recurrent Neural Networks - RNNs\n",
    "\n",
    "Recurrent Neural Networks (RNNS) are a type of neural network architecture trained on sequential or time series data that are used to make machine learning models that can make sequential predictions using previous sequence elements as inputs for the predictions. RNNs use a hidden state that helps keep track of previous inputs. This is the recurrent part of the RNN. \n",
    "\n",
    "RNNS use a encoder-decoder model. This model is best explained by the following image: \n",
    "\n",
    "![encoder/decoder model](./resources/encoder-decoder.png)\n",
    "\n",
    "For more information: \n",
    "- [IBM article](https://www.ibm.com/think/topics/recurrent-neural-networks)\n",
    "- [Blog post by Zhaozhen Xu](https://www.baeldung.com/cs/rnns-transformers-nlp)\n",
    "\n",
    "### Transformers\n",
    "\n",
    "Transformers are a type of neural network architecture that is very capable of processing natural language. Unlike RNNs, transformers do not use any recurrence or have hidden states. This means they do not operate sequentially (ie, they do not need to go through each input one at a time). They use something called self-attention. Self-attention allows the model to weigh the importance of different input tokens when making predictions. Transformers consist of encoder and decoder layers, employing multi-head self-attention mechanisms and feedback neural networks. Thanks to these features, they are able to parallelize their operations and are faster.\n",
    "\n",
    "Here is an image illustrating the transformer model.\n",
    "\n",
    "![transformer model](./resources/transformer.png)\n",
    "\n",
    "The transformer model was layed out in the 2017 seminal paper [*Attention Is All You Need*](./resources/attention.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e434997b",
   "metadata": {},
   "source": [
    "### Tokenization and Numericalization\n",
    "\n",
    "Our neural networks need to take in numbers as their inputs. We need to convert our sentences into numbers, there are two steps: \n",
    "\n",
    "- *Tokenization*: we split text up into tokens\n",
    "- *Numericalization*: convert each token into a number\n",
    "\n",
    "This process is model dependent. Each model will have a tokenizer associated with it. We'll see when developing our Dickens classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2aed32b416c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = \"ProsusAI/finbert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23c63a23ccc763e",
   "metadata": {},
   "source": [
    "```AutoTokenizer``` is a HuggingFace Transformers class that allows us to get our tokenizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3f8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4088e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz.tokenize(\"Hi! I am Sami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639e806d",
   "metadata": {},
   "source": [
    "### Our Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650030ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dickens_ds = load_dataset(\"GuillermoTBB/charles-dickens-text-classification\")\n",
    "dickens_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf088b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dickens_ds['train'], dickens_ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c387f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.to_pandas()\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98338d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(ds):\n",
    "    return tokz(ds['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337b4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = train.map(tok_func, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea4aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_df = tok_ds.to_pandas()\n",
    "tok_ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357473cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import normal, seed, uniform\n",
    "np.random.seed(42)\n",
    "\n",
    "dataloaders = tok_ds.train_test_split(0.25, seed=42)\n",
    "dataloaders = dataloaders.rename_column('label',\"labels\")\n",
    "dataloaders.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n",
    "dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebd22f",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "bs = 4\n",
    "epochs = 2\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d610de9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments( \n",
    "    output_dir=\"dickens-model\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6e503",
   "metadata": {},
   "source": [
    "#### Defining our metrics\n",
    "\n",
    "We need to define a function to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3af2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(prediction, label):\n",
    "    pred_classes = np.argmax(prediction, axis=1)\n",
    "    return np.mean(pred_classes == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e8eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(dataset):\n",
    "    return {'accuracy': accuracy(*dataset)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b322aa60",
   "metadata": {},
   "source": [
    "#### Training our model\n",
    "\n",
    "The Transformers library provides some APIs to facilitate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bce581",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForSequenceClassification, Trainer\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_nm,\n",
    "    num_labels=2,\n",
    "    id2label={0:\"not_dickens\",1:\"dickens\"},\n",
    "    label2id={\"not_dickens\":0,\"dickens\":1},)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88199a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args=args,\n",
    "                  train_dataset=dataloaders['train'],\n",
    "                  eval_dataset=dataloaders['test'],\n",
    "                  tokenizer=tokz,\n",
    "                  compute_metrics=metrics\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09d8df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfed4c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.map(tok_func, batched=True)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa53cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"./dickens-model/checkpoint-165\")\n",
    "print(\"Head bias:\", model.classifier.bias.data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af04c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pipeline API -- a general inference function\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(task=\"text-classification\", model=\"./dickens-model/checkpoint-165\", tokenizer=\"./dickens-model/checkpoint-165\", return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bd3ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"Joe was evidently made uncomfortable by what he supposed to be my loss of appetite, and took a thoughtful bite out of his slice, which he didn’t seem to enjoy. He turned it about in his mouth much longer than usual, pondering over it a good deal, and after all gulped it down like a pill. He was about to take another bite, and had just got his head on one side for a good purchase on it, when his eye fell on me, and he saw that my bread and butter was gone.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
